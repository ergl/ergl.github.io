tags: qnote imdea
title: start_transaction slowdown rundown
type: text/vnd.tiddlywiki

Quote from email sent to Deepthi

<<<
Our scenario is having several Antidote nodes spread in different data centers, but all connected to the same riak ring (so we’re not using the native antidote DC mechanism, and inter-dc communication is done using regular riak core commands).

We have separate client (basho bench) machines issuing commands to antidote  using protocol buffers, which will run a specific procedure for the client (a read-only transaction, a mixed transaction, etc). We then measure throughput and latency for each kind of command. Here’s a network topology view to make it clear:^^[topology in original email, graphviz graph here]^^

```dot
graph topology {
    subgraph site_a {
        label = "Site a";
        bench122 -- switch_1;
        bench121 -- switch_1;
        bench112 -- switch_1;
        bench111 -- switch_1;

        antidote_12 -- switch_1;
        antidote_11 -- switch_1;
    }

    subgraph site_b {
        label = "Site B";
        bench222 -- switch_1;
        bench221 -- switch_1;
        bench212 -- switch_1;
        bench211 -- switch_1;

        antidote_22 -- switch_1;
        antidote_21 -- switch_1;
    }

    antidote_11 -- antidote_21 [label="10 ms rtt"];
}
```

Our issue is that, when running on this configuration with two sites, our results are way worse than when using just a single site (think 140k tx/sec vs 250k tx/sec, for readonly transactions).

After a bit of experimentation, we’ve pinpointed the problem to the transaction coordinator supervisor pool. To give you an idea, we made the clients issue empty transaction commands (that is, antidote will create a new transaction, and then immediately commit it), and measured the different parts of the execution.

For 1k concurrent clients, we get ~0.2ms mean latency for each operation, where 0.18 ms are for start_transaction, 0.02 for commit_transaction.

For 2k concurrent clients, we jump to 8.7 ms mean latency, 8.3 ms of them spent on start_transaction, and the rest on commit.

I was curious to see if you’ve ever experienced this issue. I didn’t try to reproduce this with the current master version of Antidote, but in our fork we kept the coordinator the same way, and nevertheless it doesn’t seem like you have changed it in a meaningful way since then. Also, do you think there might be something inherently slow with the current design of the coordinator pool? On a first look it doesn’t seem like it could be, it just picks a random coordinator, and then spawns a new child process, but you never know.

Maybe it has to do with the number of concurrent clients? What’s the maximum amount of clients you’ve experimented with? (I already talked with Gonçalo Tomas, who was benchmarking antidote with FMKe, but he told me he never had this problem before).
<<<