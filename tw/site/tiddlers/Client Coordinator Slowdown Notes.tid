created: 20190226140606002
modified: 20190319105723635
tags: note pvc_client_coordinator imdea
title: Client Coordinator Slowdown Notes
type: text/vnd.tiddlywiki

Observing the node on an overloaded state show no abnormal message queues or memory consumption.

However, we get a high number of ports used: "4025/4096". This seems normal, given that we're connecting to 4k clients concurrently. Probably should bump the max number of ports in the system.

~45% of the memory used is taken up by the processes in the system.

Only ~10-12 cores are being used, and only to the ~50%.
Do we have any leeway then? Why is Erlang using only ~half of the available schedulers?
See the docs on the [[+sbt|http://erlang.org/doc/man/erl.html#+sbt]] option. For our use-case, we use `tnnps`, wich says

<<<
Schedulers are spread over hardware threads across NUMA nodes, but schedulers are only spread over processors internally in one NUMA node at a time.
<<<

Running `numactl -H` on our machines reports 2 NUMA nodes, with a memory interleave policy. This means that each node is getting half the CPU cores. This explains why our schedulers across ~half the cores.

Reducing the size of the ranch acceptor pool to 32 (number of physical cores in the target machine): no effect, but can probably reduce anyway, no need to keep more processes waiting.

Changing the number of max ports (`ERL_MAX_PORTS`) allows more clients to connect concurrently, but does not help to alleviate our problem. Antidote is //really// overloaded. Adding more client machines doesn't help with the latency.

---

Running into the problem of slow performance between single site and multi site performance.

---

''The maximum number of concurrent clients for Single Site is around 1,200''

''The maximum number of concurrent clients for Two Sites is around 2,400''

Saturation points (''ping'')

| !Clusters (Delay ms) | !Case | !Clients (Total) | !Th | !Lat (ms) |
| 1 (0) | pvc-ccoord | 300 (1,200) | ~760K | ~1.7 |
| 2 (10) | pvc-ccoord | 600 (2,400) | ~341K | ~7.11 |

---

Saturation points (''readonly'')

| !Clusters (Delay ms) | !Case | !Clients (Total) | !Th | !Lat (ms) |
| 1 (0) | pvc-ccoord | 300 (1,200) | ~607.2K | ~2.1 |
| 2 (10) | pvc-ccoord | 400 (1,600) | ~230.4k | ~7.15 |

Could it be that we're hitting a limit in the number of concurrent connections? The number of concurrent clients is about the same (1,200 vs 1,600). It's normal that with higher base latency, we would need more concurrent clients to reach the same throughput. If we need 1.2k for 600k on 0ms, how many would we need for the same on 10ms?

To test:

* Determine ''maximum concurrent connections on single site'', using a ping message (or reuse read and return empty).
** The overhead of reading from ETS shouldn't be too much, since on our experiments, that path only takes 0.125 ms, or about 2% of the execution time.
** So the number of concurrent clients should be about the same (~1.2k).
** ''Make sure the client is still spawning / committing empty transactions''
*** We don't want to change the client, should do the same allocation pattern.

* Determine ''maximum concurrent connections on two sites'' (ping again).
** If the number of concurrent clients is the same (~1.6k), then we know our limit is the number of concurrent connections, ''but only if we get a number in the same ballpark as the single site scenario''.

---

Misc

* Is it an abort issue?
* Dissect latencies
* Add more client machines
** Figure out who's getting overloaded
* Increase read replicas?
** Shut off readitem_server (old replicas)
** Shut off things we don't need (coordinator pool, etc)

---

The difference comes from the fact that Antidote is returning the entire value of the key. In our case, that's 256KB, quite a lot of data. This explains the difference we had between the client coordinator approach and the client noproxy setting (at 600 clients we had 332,796.3 / 7.483251 (noproxy) vs 230,680.2 / 10.60521 (ccoord)
