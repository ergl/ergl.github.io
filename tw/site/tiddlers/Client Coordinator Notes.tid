created: 20190308103425765
modified: 20190308103426904
tags: pvc_client_coordinator imdea note
title: Client Coordinator Notes
type: text/vnd.tiddlywiki

<div style="padding:0.5em;background: #FFF7D7;color:#000000">
''Important''<br><br>

`has_read` must be of partition_id(), not {partition_id(), node_ip()}.<br><br>

All version clocks (`vc_dep` and `vc_aggr`) have to be of partition_id(), not {partition_id(), node_ip()}
</div>


!!! Misc

<div style="padding:0.5em;background: #FFF7D7;color:#000000">
We can delay assigning a transaction id until we perform an update, or commit, since read-only transactions won't touch partitions during 2pc. We don't need a transaction id for them. 

Think about this.
</div>

!!! State

We need to keep the following state active through the entire lifetime of the application:

* Connection, wraps
** Connection socket: Send/receive commands (multiple sockets, one per physical node in cluster)
** Partition layout â€” Clients need to send read requests to appropriate partitions.

An idea about Connection. For partition layout, wee need to know a) The owning //node//, and b) the owning //partition//. We might need to put this in an external library too, but it might not be crucial. From the code, we have

```
{Partition, Node} = log_utilities:get_key_partition(Key).
```

We might, then, do, for connection, have this structure

```
-record(connection, {
    %% Same as Antidote-side code (I think)
    %% {partition_id(), node()} _is_ index_node()
    ring :: list({partition_id(), node()}),
    %% Sockets to all cluster nodes
    sockets :: orddict:orddict(node(), inet:socket())
}).
```

Then, we do

```
{Partition, Node} = log_utilities:get_key_partition(Key),
Sock = orddict:fetch(Node, Connection#connection.sockets),
%% Send message here, using Partition
```

* Transaction, wraps
** Writeset
*** Needs fast read, update
*** Able to send subset of to partition on demand
** VCdep
** VCaggr
** hasRead
** indices? -> wait until microbenchmarks are done to migrate this
** updated partitions (for commit phase)
*** might be a mapping `node() -> [partition_id()]`

!!! Commit

When the coordinator was on the database side, the messages from the partitions arrived on the process queue. On the client we have to be explicit with regards to sockets and TCP. We can't bail out of the decide phase if we get an early negative vote, since we need to flush the TCP buffers off the sockets. The original design was a map-reduce, but it got complicated fast because each node holds more than one partition, we can't split the work equally.

```
%% Partitions might be a mapping node() -> [partition_id()]
%% get_sockets is a fetch_keys-ish operation over Sockets
prepare(#connection{sockets=Socks}, #transaction{p=Partitions}) ->
    %% PartitionList might be a mapping inet:socket() -> [partition_id()]
    PartitionList = get_sockets(Socks, Partitions),
    ok = send_prepare(PartitionList),
    Outcome = collect_votes(PartitionList, CommitVC),
    ok = send_decide(PartitionList, Outcome),
    Outcome.

-spec send_prepare(_) -> ok.
send_prepare(PartitionList) ->
    lists:foreach(fun({Socket, Partitions}) ->
        lists:foreach(fun(P) -> gen_tcp:send(Socket, prepare(P) end, Partitions)
    end, PartitionList).

-spec collect_votes(_, vc()) -> vc() | {error, atom()}.
collect_votes(PartitionList, CommitVC) ->
    lists:foldl(fun({Socket, Partitions}, Acc) ->
        %% Votes might not come in same order, but we need to do N acks anyway
        %% Can't leave packets in the TCP queue
        lists:foldl(fun(_, Acc2) ->
            {ok, Reply} = gen_tcp:recv(Socket, 0),
            update_vote_acc(decode_vote(Reply, Acc2)
        end, Acc, Partitions)
    end, {ok, CommitVC}, PartitionList).

update_vote_acc(_, {false, Reason}) ->
    {false, Reason};

update_vote_acc({false, Reason}, {ok, _}) ->
    {false, Reason};

update_vote_acc({ok, Partition, Seq}, {ok, CommitVC}) ->
   {ok, pvc_vclock:set_time(Partition, Seq, CommitVC)}.

send_decide(PartitionList, Outcome) ->
    lists:foreach(fun({Socket, Partitions}) ->
        lists:foreach(fun(P) -> gen_tcp:send(Socket, decide(P, Outcome) end, Partitions)
    end, PartitionList).
```

---

''NOTE'': This whole section is incorrect, as it assumes a 1:1 mapping from sockets to partitions. In practice, each node in the cluster holds more than one partition (//vnode id//), so we need to rethink this part. We can always send in parallel across nodes, but for the same node, we need to be sequential. It is left here for reference purposes.

The prepare phase (and vote collection) can be implemented following a map-reduce(ish) approach (see after current listing). The decide phase can be a normal `foreach`, since it won't block.

```
prepare(#state{p = Partitions}) ->
    Sockets = get_sockets(Partitions),
    %% ... denotes extra state
    Result = collect_votes(fun({Partition, Socket}) ->
        wait_prepare(Socket, ...)
    end, Sockets, {ok, CommitVC}),
    %% contains any {false, Reason}?
    Outcome = get_outcome(Result),
    send_decide(Sockets, Outcome, ...).

wait_prepate(Socket, ...) ->
    gen_tcp:send(Socket, vote())
    case gen_tcp:recv(Socket) of
        {ok, Reply} ->
            decode_vote(Reply)
        {error, Reason} ->
            %% some error
            vote_error()
    end.

send_decide(Sockets, Outcome) ->
    lists:foreach(fun(Sock) -> gen_tcp:send(Sock, encode_decide(Outcome)) end, Sockets).
```

We implement `collect_votes` like follows using map-reduce. The map part just spawns a new process that will execute `wait_prepare` (a blocking call). The reduce part just iterates over the results: if the outcome if false, ignore the rest of the processes (but still pop them off the queue). If the outcome is true, modify the CommitVC with the sequence number of the vote.

```
collect_votes(Fun, List, Acc) ->
    Parent = self(),
    N = length(List),
    ok = lists:foreach(fun(Elt) ->
        spawn_link(fun() ->
            Parent ! {pmap, Fun(Elt)}
        end)
    end, List),
    collect(N, Acc).

collect(0, Acc) ->
    Acc.

collect(N, Outcome = {false, reason}) ->
  receive
    {pmap, _Res} ->
      collect(N - 1, Outcome)
  end;

collect(N, Outcome = {ok, CommitVC}) ->
    receive
        {pmap, Res} ->
            case Res of
                {ok, Partition, Seq} ->
                    collect(N - 1, set_time(Partition, Seq, CommitVC));
                False={false, Reason} ->
                    collect(N - 1, False)
            end
    end.
```

!!! Reads

Reads can be implemented similarly to the Commit, above. Remember, that reads ''must'' be performed sequentially, even if reading more than one key (Transaction state needs to be updated between reads).

!!! Messages

```
Connect {}

Connected {
    ...
}

ReadRequest {
    Partition
    Key
    VCaggr // needs [i] and [w] \forall w . updated
    hasRead // needs [i] and [w] \forall w . true
}

ReadReturn {
    abort | {
        Value
        VersionVC
        MaxVC
    }
}

Prepare {
    Partition
    TransactionId // for id of entry in CommitQueue
    PartitionWriteSet // for write-write conflict check
    VCdep[Partition] // just integer, for stale check
}

Vote {
    Partition // sending Partition
    {false, Reason} | {true, SeqNumber}
}

Decide {
    Partition
    TransactionId // for CommitQueue update/removal
    false | {true, commitVC} // whole VC needed
}
```
