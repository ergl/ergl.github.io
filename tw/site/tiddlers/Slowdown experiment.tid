created: 20190319105723541
modified: 20190329104139709
tags: note imdea
title: Slowdown experiment
type: text/vnd.tiddlywiki

!! The Problem

<<<
We distinguish two scenarios: one with no latency added between sites (0.25 ms rtt), and another with an extra 10ms added (10ms rtt). Several 5-minute runs are performed, each with an increasing number of concurrent connections (from 50 up to 1000, so each server is holding 200 to 4000 concurrent connections).

The results are [..]:

|! Scenario |! Max Th | !Median Latency |
| No latency | 627,434.9 | 7.39232 |
| 10ms rtt | 340,299.5 | 12.05045 |
<<< [[Slowdown experiment details]]

!! Explanation

Most information here is sourced from a  [[great email by Jesper|http://erlang.org/pipermail/erlang-questions/2019-March/097496.html]] on the Erlang mailing list.

Under a general case, an increase in latency doesn't necessarily lead to a decrease in maximum throughput. Under the same circumstances, only the base latency increases, but the saturation point of the system is the same. So why this doesn't hold in our case?

The problem resides in how we do our benchmark: a client sends a request, and waits until it receives a reply to proceed to the next operation. In the context of a single transaction, this makes sense, because we need to know the results of a previous operation in order to issue the next one. Under such a system (known as "[[Stop & Wait]] protocol"), there is at most a single packet on the wire per connection, and so the maximum number of packets per second per connection is directly tied to the latency of the medium.

<<<
Some napkin math: Suppose you have 1 connection. You have a lower RTT of 10ms. At most, this is 100 req/s on that connection. Suppose we have 500 of those connections. Then the maximal req/s is 500*100 = 50,000. 
<<<

And so, 

$$
Th_{max} =  \frac{1000 \times \mathit{connections}}{\mathit{Latency\ (ms)}}
$$

For our case, with a 10ms RTT and 4k connections, the max throughput is 400,000 reqs/sec, which is what we observe in the benchmarks.

Another, unrelated problem, is the one [[Coordinated Omission]]; but this is more the fault of `basho_bench`, so there's less we can do about it.

!! Solutions

Move away from a stop & wait protocol (which is something we didn't even intend to do in the first place). There are two approaches here:

* Batching. Collect multiple requests and send them all off at the same time. This gets around the delay constant by having several requests on the wire. It can also let the server process multiple connections at the same time (if possible). To do this: when the first request arrives, set a [[cork|TCP Cork in Erlang]] of 5ms. Upon having read either X reqs or the timer triggering, process whatever you have in the batch. Used by Kafka and TensorFlow.

* Pipelining. Send multiple requests back to back, so they are all in the network. This requires more changes on the client side, as we need to interleave our receive loop with new requests being sent, so we can keep the work going. To do this, we need to tag messages with unique identifiers so they can be told apart (allowing out-of-order processing). See the [[9P|https://en.wikipedia.org/wiki/9P_(protocol)]] protocol from Plan 9, RabbitMQ/AMQP's correlation IDs, or HTTP/2. Quick implementation: Cowboy/Gun combo works wonders.
