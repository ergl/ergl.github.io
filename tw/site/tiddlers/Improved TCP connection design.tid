created: 20190329104449368
modified: 20190331195227529
tags: note pvc_client_coordinator imdea
title: Improved TCP connection design
type: text/vnd.tiddlywiki

Because of the problems outlined in [[Slowdown experiment]], we need to rethink the way we connect to the server from the client coordinator library (see the [[design notes|Client Coordinator Notes]]).

Solutions proposed: a) batching, and b) pipelining. The following design combines the two approaches, as well as doing connection multiplexing.

!! How it was before

A coordinator connection structure (`pvc:connection`) is a wrapper around a collection of physical TCP connections, each connecting to a server in the cluster. On the client side, a `connection` is used per worker thread, which means that each clients maintains its own TCP connection (or multiple). This is a waste of ports, as well as leading to the problems detailed in the [[experiment|Slowdown experiment]] mentioned above.

!! Hot it is now

On the client side, nothing changes, a `pvc:connection` is still used for the library calls, but internally uses a shared multiplexed socket, managed by a new module. Now, a connection type is a collection of managed connections. This managed connections might be obtained from a resource pool (non exclusive), or by supplying them at construction time.

!! Managed connections

Instead of using a socket and `gen_tcp` directly, we'll use a `connection_handle` type, which is basically the pid of a `gen_server` instance, the owner of the connection.

A managed connection can be shared between multiple users. Internally, a managed connection is a gen server, which handles an outbound queue and a message id table, mapping ids to the worker processes that requested to send a message. The following sequence diagram illustrates the architecture:

<<hl "Rename the types, probably, could be confusing">>

{{$:/ergl/images/improved_pvc_connection_diagram.svg}}

Here, the //User// would be a `pvc:connection` structure, and the //Connection// would be the managed connection.

In Erlang-ish pseudocode, the `send` operation would be:


```erlang
%% With connection state:
#state {
    socket :: inet:socket(),
    cork_timer = not_set :: timer:tref() | not_set,
    cork_len = 5 :: non_neg_integer(),
    msg_owner_t :: ets:tid(),
    buffer :: (???), % Either iodata() | binary()
    buffer_watermark = 500 :: non_neg_integer(),
    buffer_len = 0 :: non_neg_integer()
}.

start(...) ->
    State.cork_timer = timer:send_after(cork_len, self(), flush_buffer).
    
%% ...

send(Pid, <<Id:8, Msg/binary) ->
    gen_server:cast(Pid, {queue, self(), Msg}).

%% ...
 
handle_cast({queue, Pid, <<Id:8, _/binary>>=Msg}, State) ->
    append(Msg, State.buffer),
    State.buffer_len++
    ets:isert(State.msg_owner_t, {Id, Pid}),
    if State.buffer_len == buffer_watermark {
        flush_buffer(State)
    } else {
        maybe_rearm_timer(State)
    }
    {noreply, State}.
    
    
flush_buffer(State) ->
    if State.cork_timer != not_set {
        timer:stop(State.cork_timer)
    }
    gen_tcp:send(State.socket, State.buffer),
    empty(State.buffer)
    buffer_len = 0.
    
maybe_rearm_timer(State) ->
    if State.cork_timer == not_set {
        State.cork_timer = timer:send_after(cork_len, self(), flush_buffer).
    }
    
handle_info(flush_buffer, State) ->
    flush_buffer(State),
    State.cork_timer = timer:send_after(cork_len, self(), flush_buffer).
```


---

!! Random notes

Immediately put it into an outbound queue (msg_buffer), and start a timer for `cork_length` ms from now (if a timer is not already active).

When either: a) timer expires, or b) buffer_len == buffer_watermark, the sever will send the entire buffer through the socket.

On receiving a message from the socket (set on {active, 1}), get the msg id from the owner table. If there is no match, drop the message on the floor. If there is match, `erlang:send` back to the owner.

Questions:

How do we keep the buffer? Can we send it on a single call to send and still receive it on the server side as separate messages? (Do we want this? maybe we can process multiple messages in a single go)

